# Vietnamese Word Tokenize ![](https://img.shields.io/badge/F1-94%25-red.svg)

The code in this repository are heavily adapted from a research project
[word_tokenize](https://github.com/undertheseanlp/word_tokenize) belongs to [Vu Anh](https://github.com/rain1024).
In this repository, I've try to refactor his code and try to make sense of how to preprocess data and train the models.
